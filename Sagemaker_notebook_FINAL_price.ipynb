{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f9c119",
   "metadata": {},
   "source": [
    "# SageMaker Notebook Instance to Forecast Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408987f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "import numpy as np\n",
    "import io\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d824b",
   "metadata": {},
   "source": [
    "# Data Extraction and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa6680",
   "metadata": {},
   "source": [
    "- The code splits the 'time' column into 'date' and 'hour' columns and further splits the 'hour' column to exclude timezone information. \n",
    "\n",
    "\n",
    "- The 'time' column is then dropped since it is no longer needed. \n",
    "\n",
    "\n",
    "- Any rows with missing values in the variable `'total load actual'` are then dropped to ensure that the same amount of information is used to train this model.\n",
    "\n",
    "\n",
    "- Any rows with missing values in the target variable <b>`'price actual'`</b> are then dropped. \n",
    "\n",
    "\n",
    "- Finally, the code creates two objects, X and y, which contain the independent and dependent variables, respectively, for use in a predictive mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8538df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Datasets from S3 Bucket\n",
    "df_energy = pd.read_csv(\"s3://bucket-aws-group-project-load-datasets-from-local/energy_dataset.csv\")\n",
    "df_weather = pd.read_csv(\"s3://bucket-aws-group-project-load-datasets-from-local/weather_features.csv\")\n",
    "\n",
    "# Merging datasets\n",
    "df = pd.merge(df_energy, df_weather, left_on='time', right_on='dt_iso', how='right')\n",
    "\n",
    "# Dropping non-relevant columns\n",
    "# The columns dropped have been removed because they either:\n",
    "# - contained all 0s instances\n",
    "# - contained all NAs instances\n",
    "# - caused target leackage because they contained the already forecasted value by the Transmission Service Operator (TSO)\n",
    "df.drop(columns=['dt_iso', 'generation fossil coal-derived gas',\n",
    "                 'generation fossil oil shale', 'generation fossil peat', \n",
    "                 'generation geothermal', 'generation marine', 'generation wind offshore', \n",
    "                 'generation hydro pumped storage aggregated',\n",
    "                 'forecast wind offshore eday ahead', 'weather_description', \n",
    "                 'weather_id', 'weather_icon', 'forecast solar day ahead', \n",
    "                 'forecast wind onshore day ahead', 'total load forecast'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Split the 'time' column into 'date' and 'hour' columns\n",
    "df[['date', 'hour']] = df['time'].str.split(' ', expand=True)\n",
    "\n",
    "# Split the 'hour' column further to exclude the timezone information\n",
    "df['hour'] = df['hour'].str.split('+', expand=True)[0]\n",
    "\n",
    "# Now we can drop the 'time' column, since we won't need it anymore to merge the tables\n",
    "df.drop('time', axis=1, inplace=True)\n",
    "\n",
    "# Drop rows with NAs in 'total load actual' to preserve same amount of data for this model.\n",
    "df = df.dropna(subset=['total load actual'])\n",
    "\n",
    "# Drop NAs in the target variable\n",
    "df = df.dropna(subset=['price actual'])\n",
    "\n",
    "# Create X matrix and y array\n",
    "y = df['price actual']\n",
    "X = df.drop('price actual', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226608c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoded columns for city_name\n",
    "X = pd.get_dummies(X, columns=['city_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1381271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoded columns for weather_main\n",
    "X = pd.get_dummies(X, columns=['weather_main'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjust the date column\n",
    "X['date'] = pd.to_datetime(X['date'])\n",
    "# Double-validate the correct format of the date\n",
    "X['date'] = X['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Extract the year, month, and day components from the 'date' column\n",
    "X['year'] = X['date'].apply(lambda x: int(x[:4]))\n",
    "X['month'] = X['date'].apply(lambda x: int(x[5:7]))\n",
    "X['day'] = X['date'].apply(lambda x: int(x[8:10]))\n",
    "# Drop original column\n",
    "X.drop('date', axis=1, inplace=True)\n",
    "\n",
    "### Adjust the time column\n",
    "from datetime import datetime\n",
    "X['hour'] = X['hour'].apply(lambda x: datetime.strptime(x, '%H:%M:%S').time())\n",
    "# We need to use apply since it's a Series\n",
    "X['hour'] = X['hour'].apply(lambda x: x.hour)\n",
    "\n",
    "# Add target variable as first column\n",
    "X = pd.concat([y, X], axis=1)\n",
    "\n",
    "# Split the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(X, test_size=0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cae74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9626b0",
   "metadata": {},
   "source": [
    "# Uploading Channels with S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74056241",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "def upload_to_s3(df, bucket, filename):\n",
    "    \n",
    "    placeho# Connecting so S3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#The function first creates an empty string buffer object using io.StringIO(), which will be used to hold the contents of the DataFrame in CSV format. \n",
    "#The to_csv() method is called on the DataFrame, with the header and index arguments set to False to exclude these from the CSV output. \n",
    "#The resulting CSV data is then written to the string buffer.\n",
    "#Next, an S3 Object instance is created using the boto3 library, with the bucket and filename arguments passed to the constructor. \n",
    "#The put() method is called on the Object instance with the contents of the string buffer passed as the Body argument. \n",
    "\n",
    "def upload_to_s3(df, bucket, filename):\n",
    "    \n",
    "    placeholder = io.StringIO()\n",
    "    df.to_csv(placeholder, header=False, index=False)\n",
    "    object = s3.Object(bucket, filename)\n",
    "    object.put(Body=placeholder.getvalue())\n",
    "    \n",
    "###This uploads the file to the S3 bucket with the specified filename.lder = io.StringIO()\n",
    "    df.to_csv(placeholder, header=False, index=False)\n",
    "    object = s3.Object(bucket, filename)\n",
    "    object.put(Body=placeholder.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8b80f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_s3(train, 'bucket-for-sagemaker-from-notebook', 'sagemaker-data-train.csv')\n",
    "upload_to_s3(test, 'bucket-for-sagemaker-from-notebook', 'sagemaker-data-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b4571",
   "metadata": {},
   "source": [
    "# Instanciating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ccdd92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line retrieves the execution role associated with the current SageMaker notebook instance. \n",
    "# This role is used to grant permissions to the SageMaker training job to access other AWS services like S3.\n",
    "role = sagemaker.get_execution_role()\n",
    "# This line gets the AWS region name where the notebook instance is running. \n",
    "# This information is used to set the region for the SageMaker training job.\n",
    "region_name = boto3.Session().region_name\n",
    "#container = get_image_uri(region_name, 'xgboost', '0.90-1')  # Old version\n",
    "\n",
    "# This line retrieves the Amazon ECR container location for the XGBoost algorithm image. \n",
    "# It uses the sagemaker.image_uris.retrieve() method to get the image URI, passing in the region, the name of the algorithm, and the version number.\n",
    "container = sagemaker.image_uris.retrieve('xgboost', region_name, version='0.90-1')\n",
    "# This line sets the S3 output location for the trained model artifacts. \n",
    "output_location = 's3://bucket-for-sagemaker-from-notebook/'\n",
    "\n",
    "# This line defines a dictionary of hyperparameters to be passed to the XGBoost algorithm for training. \n",
    "# In this case, it sets the number of rounds to 200 and the objective to 'reg:squarederror', which is a regression objective that minimizes the mean squared error.\n",
    "hyperparams = {\n",
    "    'num_round': '200',\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "\n",
    "# This line creates a SageMaker estimator object, which encapsulates information about the training job to be launched. \n",
    "# It specifies the Docker container for the XGBoost algorithm image, the execution role, the number of instances to use, the instance type, the S3 output path for the trained model, the hyperparameters to use for training, and the SageMaker session object. \n",
    "# The session object is used to create the training job in the specified AWS region.\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    output_path=output_location,\n",
    "    hyperparameters=hyperparams,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87fc51f",
   "metadata": {},
   "source": [
    "# Setting Up Input Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e774ac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# This line creates an S3 input channel for the training data. \n",
    "# It uses the sagemaker.session.s3_input() method to create an input object, passing in the S3 location of the training data and the content type of the data (in this case, CSV). \n",
    "# The resulting train_channel object represents the input data for the training job.\n",
    "train_channel = sagemaker.session.s3_input(\n",
    "    's3://bucket-for-sagemaker-from-notebook/sagemaker-data-train.csv',\n",
    "    content_type='text/csv'\n",
    ")\n",
    "\n",
    "# This line creates a separate S3 input channel for the validation data. \n",
    "# It follows the same pattern as the previous line, but specifies the location of the validation data and a separate val_channel object.\n",
    "val_channel = sagemaker.session.s3_input(\n",
    "    's3://bucket-for-sagemaker-from-notebook/sagemaker-data-test.csv',\n",
    "    content_type='text/csv'\n",
    ")\n",
    "\n",
    "# This line creates a dictionary object called channels_for_training that maps channel names to input channels. \n",
    "# In this case, the dictionary has two entries: 'train', which maps to the train_channel object representing the training data, and 'validation', which maps to the val_channel object representing the validation data.\n",
    "channels_for_training = {\n",
    "    'train': train_channel,\n",
    "    'validation': val_channel\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab76e5a0",
   "metadata": {},
   "source": [
    "# Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "371f556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-03-13-14-14-45-478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-03-13 14:14:45 Starting - Starting the training job.......\n",
      "2023-03-13 14:15:25 Starting - Preparing the instances for training................\n",
      "2023-03-13 14:16:49 Downloading - Downloading input data.....\n",
      "2023-03-13 14:17:19 Training - Downloading the training image.....\n",
      "2023-03-13 14:17:50 Training - Training image download completed. Training in progress.....\n",
      "2023-03-13 14:18:16 Uploading - Uploading generated training model..\n",
      "2023-03-13 14:18:32 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs=channels_for_training, logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "426c18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator._current_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cac696",
   "metadata": {},
   "source": [
    "# Getting the Metrics of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a389a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = sagemaker.analytics.TrainingJobAnalytics(\n",
    "    estimator._current_job_name,\n",
    "    metric_names=['train:rmse', 'validation:rmse']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bbe1a9",
   "metadata": {},
   "source": [
    "# Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08203937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2023-03-13-14-18-33-729\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2023-03-13-14-18-33-729\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2023-03-13-14-18-33-729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "# Deploy model\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', serializer=sagemaker.serializers.CSVSerializer())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a298583",
   "metadata": {},
   "source": [
    "# Running and Storing Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea97e4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">The purpose of this function is to <b>generate new data points</b> for testing and evaluating a predictive model trained on the original dataset. By randomly generating values for the different features, the function simulates real-world variability in the data and allows the model to be tested on a range of different input values.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf62019",
   "metadata": {},
   "source": [
    "This function generates a dictionary object with randomly generated values for different features related to energy and weather data.\n",
    "\n",
    "The function creates a dictionary object called new_data with 58 keys, each of which corresponds to a different feature. The values of these features are randomly generated using numpy's `np.random.randint()` function, which generates integers within specified ranges.\n",
    "\n",
    "The function also sets the values of two features, `'city_name'` and `'weather_main'`, based on randomly generated indices. The corresponding element in the new_data dictionary is then set to 1, depending on the values of these indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "def get_new_data():\n",
    "    \n",
    "    \n",
    "    \n",
    "    new_data = {\n",
    "    'generation biomass': np.random.randint(0, 600),\n",
    "    'generation fossil brown coal/lignite': np.random.randint(0, 1000),\n",
    "    'generation fossil gas': np.random.randint(0, 13000),\n",
    "    'generation fossil hard coal': np.random.randint(0, 8000),\n",
    "    'generation fossil oil': np.random.randint(0, 500),\n",
    "    'generation hydro pumped storage consumption': np.random.randint(0, 4000),\n",
    "    'generation hydro run-of-river and poundage': np.random.randint(0, 2000),\n",
    "    'generation hydro water reservoir': np.random.randint(0, 9000),\n",
    "    'generation nuclear': np.random.randint(2000, 7000),\n",
    "    'generation other': np.random.randint(0, 100),\n",
    "    'generation other renewable': np.random.randint(0, 100),\n",
    "    'generation solar': np.random.randint(0, 6000),\n",
    "    'generation waste': np.random.randint(0, 350),\n",
    "    'generation wind onshore': np.random.randint(0, 18000),\n",
    "    'price day ahead': np.random.randint(2, 100),\n",
    "    'temp': np.random.randint(250, 275),\n",
    "    'temp_min': np.random.randint(150, 250),\n",
    "    'temp_max': np.random.randint(275, 350),\n",
    "    'pressure': np.random.randint(0, 5000),\n",
    "    'humidity': np.random.randint(10, 100),\n",
    "    'wind_speed': np.random.randint(0, 30),\n",
    "    'wind_deg': np.random.randint(0, 360),\n",
    "    'rain_1h': np.random.randint(0, 8),\n",
    "    'rain_3h': 0,\n",
    "    'snow_3h': 0,\n",
    "    'clouds_all': np.random.randint(0, 90),\n",
    "    'year': np.random.randint(2016, 2018),\n",
    "    'month': np.random.randint(1, 12), \n",
    "    'day': np.random.randint(1, 28),\n",
    "    'hour': np.random.randint(0, 23),\n",
    "    'city_name_ Barcelona': 0,\n",
    "    'city_name_Bilbao': 0,\n",
    "    'city_name_Madrid': 0,\n",
    "    'city_name_Seville': 0,\n",
    "    'city_name_Valencia': 0,\n",
    "    'weather_main_clear': 0,\n",
    "    'weather_main_clouds': 0,\n",
    "    'weather_main_drizzle': 0,\n",
    "    'weather_main_dust': 0,\n",
    "    'weather_main_fog': 0,\n",
    "    'weather_main_haze': 0,\n",
    "    'weather_main_mist': 0,\n",
    "    'weather_main_rain': 0,\n",
    "    'weather_main_smoke': 0,\n",
    "    'weather_main_snow': 0,\n",
    "    'weather_main_squall': 0,\n",
    "    'weather_main_thunderstorm': 0, \n",
    "    'total load actual': np.random.randint(18000, 40000)}\n",
    "    \n",
    "    # Generate random indices for city_name and weather_main\n",
    "    city_index = np.random.randint(0, 5)\n",
    "    weather_index = np.random.randint(0, 11)\n",
    "    # Set the corresponding element in the new_data dictionary to 1\n",
    "    if city_index == 0:\n",
    "        new_data['city_name_ Barcelona'] = 1\n",
    "    elif city_index == 1:\n",
    "        new_data['city_name_Bilbao'] = 1\n",
    "    elif city_index == 2:\n",
    "        new_data['city_name_Madrid'] = 1\n",
    "    elif city_index == 3:\n",
    "        new_data['city_name_Seville'] = 1\n",
    "    else:\n",
    "        new_data['city_name_Valencia'] = 1\n",
    "\n",
    "    if weather_index == 0:\n",
    "        new_data['weather_main_clear'] = 1\n",
    "    elif weather_index == 1:\n",
    "        new_data['weather_main_clouds'] = 1\n",
    "    elif weather_index == 2:\n",
    "        new_data['weather_main_drizzle'] = 1\n",
    "    elif weather_index == 3:\n",
    "        new_data['weather_main_dust'] = 1\n",
    "    elif weather_index == 4:\n",
    "        new_data['weather_main_fog'] = 1\n",
    "    elif weather_index == 5:\n",
    "        new_data['weather_main_haze'] = 1\n",
    "    elif weather_index == 6:\n",
    "        new_data['weather_main_mist'] = 1\n",
    "    elif weather_index == 7:\n",
    "        new_data['weather_main_rain'] = 1\n",
    "    elif weather_index == 8:\n",
    "        new_data['weather_main_smoke'] = 1\n",
    "    elif weather_index == 9:\n",
    "        new_data['weather_main_snow'] = 1\n",
    "    else:\n",
    "        new_data['weather_main_thunderstorm'] = 1\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce3b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new row of data as a pandas dataframe\n",
    "X_new = pd.DataFrame([get_new_data()])\n",
    "predictor.predict(X_new.iloc[0])\n",
    "import json\n",
    "# Convert the predictions to JSON format\n",
    "predictions = predictor.predict(X_new.iloc[0]).decode('utf-8')\n",
    "predictions_json = json.dumps(predictions)\n",
    "predictions_final = float(predictions_json.strip('\" '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba991e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new['predicted_value'] = predictions_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dataframe to store predictions\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Run Loop\n",
    "for i in range(10000):\n",
    "    # Generate new row of Independent Variables for this iteration\n",
    "    X_new = pd.DataFrame([get_new_data()])\n",
    "    \n",
    "    ### Get the prediction for this data\n",
    "    # Convert the predictions to JSON format. Decode from binary.\n",
    "    predictions = predictor.predict(X_new.iloc[0]).decode('utf-8')\n",
    "    # Store JSON file\n",
    "    predictions_json = json.dumps(predictions)\n",
    "    # Store final prediction as floating, after removing string characters in excess.\n",
    "    predictions_final = float(predictions_json.strip('\" '))\n",
    "    \n",
    "    # Add the prediction to the new data DataFrame\n",
    "    X_new['predicted_value'] = predictions_final\n",
    "    \n",
    "    # Append the new data to the results DataFrame\n",
    "    results_df = results_df.append(X_new, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3559e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ede795",
   "metadata": {},
   "source": [
    "# Uploading Predictions to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa34ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the S3 bucket and key to upload to\n",
    "bucket_name = 'buket-containing-predictions-for-lambda'\n",
    "key = 'predictions_10000_price.csv'\n",
    "\n",
    "# write the DataFrame to a local CSV file\n",
    "csv_buffer = results_df.to_csv(index=False)\n",
    "\n",
    "# upload the CSV file to S3\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket_name).put_object(Key=key, Body=csv_buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
